---
title: "Scraping data from deep web markets"
author: "Nick Janetos"
date: '`r Sys.Date()`'
output: 
    html_document:
        theme: cosmo
        highlight: tango
vignette: |
  %\VignetteIndexEntry{Vignette Title} %\VignetteEngine{knitr::rmarkdown} %\VignetteEncoding{UTF-8}
---

# Introduction

This document describes the process through which I archive and scrape data from the 'Deep Web'. Broadly, the process consists of three steps:

1. An automated archiver connects, using the [Tor network](https://www.torproject.org/), to a list of online marketplaces on a weekly basis, and for each marketplace, crawls through the links, storing any relevant pages, and archives the raw HTML source for the page along with the date it was downloaded and the site path it took to get to the page. The source code for this narcotics archiver, or 'narchiver', can be found [here](https://github.com/njanetos/narchiver). 
2. A data scraping program is later used to go through the archives of HTML pages. It attempts to categorize each listing by searching for keywords (e.g., finding 'mdma', 'xtc', 'ecstasy', and 'molly' in the page will result in a categorization of 'mdma'), and creates a list of 'price points'. Each price point represents a particular price for a particular listing at a particular time for some particular amount. The source code for the data scraping program can be found [here](https://github.com/njanetos/dataextractor). 

## Connecting to Tor

Connecting to Tor is done by installing the `tor` package, then connecting to it as one would to a normal HTTP connection through Java, but on a different port than port 80. Therefore, any scraper which can scrape over HTTP can also scrape over Tor, simply by routing the ports appropriately. In particular, for individuals interested in scraping their own deep web sites, it is not necessary to write a custom scraper as I did, an existing alternative such as [scrapy](http://scrapy.org/) will work. 

## Marketplaces scraped

Marketplaces are added as I get the time to add them, and disappear when their owners abscond with cash held in escrow, or decide to close the site out of fear of law enforcement, or are arrested by law enforcement. The following is a complete list of all sites archived Sites with a strikethrough have disappeared, sites in bold are available in the database. 

* ~~**Silkroad 2.0**~~ (September 2014 - November 2014)
* ~~**Marketplace**~~ (September 2014 - November 2014)
* ~~**Evolution**~~ (November 2014 - March 2015)
* ~~Black Bank~~ (November 2014 - March 2015)
* ~~Dream~~ (November 2014 - March 2015)
* **Agora** (November 2014 - Today)
* Nucleus (November 2014 - Today)

## Raw data

The archiving program begins at the top level of the site, finds all the links on the page, then follows each one in turn, repeating this process recursively. It ignores links outside the site, and special links (such as, e.g., `/logout` or `/contact`) specified in a initialization file. (The github page contains the various initialization files.) It then compresses the raw HTML and stores it on an external server along with a timestamp and the path which the archiver took through the site to download it (e.g., `/index`-`/index/listings`-`/index/listings/mdma`-`/index/listings/mdma/xtc-220mg-cheap`). As of the time of this writing, there is approximately 20 gb of compressed raw HTML.

## Categorizing listings

Once the HTML has been downloaded, a data scraping program processes each HTML file. For each site available in the database, I implemented methods in Java which try to find the price, the name of the listing, the vendor offering the product, and to categorize the product. Finding the first three involves looking carefully at the HTML for each listing, and discovering some identifying tag or id. For example, the price might always be contained in the HTML div element `<div id='price'> 0.34425 BTC </div>`. I use `jsoup` to extract the string `0.34425 BTC`, then write custom code for each marketplace to extract the number `0.34425`, stored under `price`, and the string `BTC`, which is stored under `denomination`.

Categorizing the listings presents a greater challenge, due to the great variety of terminology and lack for all of the marketplaces of any standardized categorization system. A pack of 20 tablets containing 220mg of MDMA might be listed as any one of the following:

* 220mg x20 XTC
* ECSTASY TOP QUALITY 220MG 20 PILLS
* mdma pills 220mg 20
* NETHERLANDS XTC 220 MG X20

The following fields need to be filled in by the scraper: `amount`, which here should be 220, `units`, which here should be 'mg', `mult`, which here should be 20, and `category`, which here should be `MDMA`. The class `UnitExtractor` contains the code I use to try to extract `amount`, `units`, and `mult`, while the listings are categorized by running a regex comparison against a list I compiled of drug categories and their synonyms / slang terms (`categories.json`). This is not guaranteed to always work, currently, casual sampling of the categorizations reveals an approximately 50% success rate. Among listings which are successfully categorized (uncategorized listings have a -1 in the `mult` field or `category = 'UNCATEGORIZED'`), the number of listings which are _correctly_ categorized seems to be much larger, but how much larger is still to be determined. 